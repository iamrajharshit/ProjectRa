{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPG0pYqsrq2f051uqJ9FYB7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch"],"metadata":{"id":"9vHKaiLKzyVz","executionInfo":{"status":"ok","timestamp":1726909216312,"user_tz":-330,"elapsed":7395,"user":{"displayName":"Harshit Raj","userId":"00461961014179132180"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"iwH4utkBvLtv","executionInfo":{"status":"ok","timestamp":1726909250481,"user_tz":-330,"elapsed":446,"user":{"displayName":"Harshit Raj","userId":"00461961014179132180"}}},"outputs":[],"source":["def linear_q_with_scale_and_zero_point(\n","    r_tensor, scale, zero_point, dtype=torch.int8):\n","    \"\"\"\n","    Performs simple linear quantization given\n","    the scale and zero-point.\n","    \"\"\"\n","\n","    # scale tensor and add the zero point\n","    scaled_and_shifted_tensor = r_tensor / scale + zero_point\n","\n","    # round the tensor\n","    rounded_tensor = torch.round(scaled_and_shifted_tensor)\n","\n","    # we need to clamp to the min/max value of the specified dtype\n","    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n","    q_tensor = rounded_tensor.clamp(q_min, q_max).to(dtype)\n","    return q_tensor\n","\n","\n","def get_q_scale_symmetric(tensor, dtype=torch.int8):\n","    r_max = tensor.abs().max().item()\n","    q_max = torch.iinfo(dtype).max\n","\n","    # return the scale\n","    return r_max/q_max\n","\n","\n","def linear_q_symmetric(tensor, dtype=torch.int8):\n","    scale = get_q_scale_symmetric(tensor)\n","\n","    quantized_tensor = linear_q_with_scale_and_zero_point(tensor,\n","                                                     scale=scale,\n","                   # in symmetric quantization zero point is = 0\n","                                                    zero_point=0,\n","                                                      dtype=dtype)\n","\n","    return quantized_tensor, scale"]},{"cell_type":"markdown","source":["## Linear Quantization: Inference\n","- W8A32 means weights in 8-bits and activations in 32-bits.\n","- For simplicity, the linear layer will be without bias"],"metadata":{"id":"nE_nQI6X0DHc"}},{"cell_type":"code","source":["def quantized_linear_W8A32_without_bias(input, q_w, s_w, z_w):\n","    assert input.dtype == torch.float32\n","    assert q_w.dtype == torch.int8\n","\n","    dequantized_weight = q_w.to(torch.float32) * s_w + z_w\n","    output = torch.nn.functional.linear(input, dequantized_weight)\n","\n","    return output"],"metadata":{"id":"l4g5gfCGz1SM","executionInfo":{"status":"ok","timestamp":1726909285770,"user_tz":-330,"elapsed":409,"user":{"displayName":"Harshit Raj","userId":"00461961014179132180"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["input = torch.tensor([1, 2, 3], dtype=torch.float32)"],"metadata":{"id":"Tt8U8XuS0IFr","executionInfo":{"status":"ok","timestamp":1726909290601,"user_tz":-330,"elapsed":408,"user":{"displayName":"Harshit Raj","userId":"00461961014179132180"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["weight = torch.tensor([[-2,   -1.13, 0.42],\n","                       [-1.51, 0.25, 1.62],\n","                       [0.23,  1.35, 2.15]])"],"metadata":{"id":"XlztOZtm0JQh","executionInfo":{"status":"ok","timestamp":1726909294983,"user_tz":-330,"elapsed":409,"user":{"displayName":"Harshit Raj","userId":"00461961014179132180"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["q_w, s_w  = linear_q_symmetric(weight)"],"metadata":{"id":"0Y3Renxz0KVj","executionInfo":{"status":"ok","timestamp":1726909302693,"user_tz":-330,"elapsed":2,"user":{"displayName":"Harshit Raj","userId":"00461961014179132180"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["q_w"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Y9iir-H0Lh_","executionInfo":{"status":"ok","timestamp":1726909309195,"user_tz":-330,"elapsed":670,"user":{"displayName":"Harshit Raj","userId":"00461961014179132180"}},"outputId":"a373efab-6516-4237-cf3c-c1948561af4f"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-118,  -67,   25],\n","        [ -89,   15,   96],\n","        [  14,   80,  127]], dtype=torch.int8)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["s_w"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eKrAUt8P0Nvz","executionInfo":{"status":"ok","timestamp":1726909316692,"user_tz":-330,"elapsed":468,"user":{"displayName":"Harshit Raj","userId":"00461961014179132180"}},"outputId":"55c17cb1-7ed5-4464-bf9b-28b74afdc117"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.016929134609192376"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["output = quantized_linear_W8A32_without_bias(input,\n","                                             q_w,\n","                                             s_w,\n","                                             0)"],"metadata":{"id":"wVBjfvCP0Pnz","executionInfo":{"status":"ok","timestamp":1726909323603,"user_tz":-330,"elapsed":457,"user":{"displayName":"Harshit Raj","userId":"00461961014179132180"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["print(f\"This is the W8A32 output: {output}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V_v2g2Y70RTN","executionInfo":{"status":"ok","timestamp":1726909327916,"user_tz":-330,"elapsed":416,"user":{"displayName":"Harshit Raj","userId":"00461961014179132180"}},"outputId":"18191ae0-53ef-42ee-f9c5-e6f171deae75"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["This is the W8A32 output: tensor([-2.9965,  3.8768,  9.3957])\n"]}]},{"cell_type":"code","source":["fp32_output = torch.nn.functional.linear(input, weight)"],"metadata":{"id":"nenvyNae0SX4","executionInfo":{"status":"ok","timestamp":1726909336155,"user_tz":-330,"elapsed":2,"user":{"displayName":"Harshit Raj","userId":"00461961014179132180"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["print(f\"This is the output if we don't quantize: {fp32_output}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qXr4ssba0VaX","executionInfo":{"status":"ok","timestamp":1726909353545,"user_tz":-330,"elapsed":418,"user":{"displayName":"Harshit Raj","userId":"00461961014179132180"}},"outputId":"075de62e-b297-42dc-a6a3-91f1a5f48b50"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["This is the output if we don't quantize: tensor([-3.0000,  3.8500,  9.3800])\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"n5nlrlnF0TyB"},"execution_count":null,"outputs":[]}]}