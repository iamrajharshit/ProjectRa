{"cells":[{"cell_type":"markdown","metadata":{"id":"W6tCv-A_bJVb"},"source":["# Linear Quantization\n","\n","\n","\n","#### Libraries to install\n","```Python\n","!pip install transformers==4.35.0\n","!pip install quanto==0.0.11\n","!pip install torch==2.1.1\n","```"]},{"cell_type":"markdown","metadata":{"id":"CMCnRV70ssMD"},"source":["## T5-FLAN\n","For tokenizing install sentencepiece\n","```Python\n","!pip install sentencepiece==0.2.0\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"9_VHiNx_ssME"},"source":["### Without Quantization"]},{"cell_type":"code","execution_count":null,"metadata":{"height":30,"id":"HhwPLQXUssME"},"outputs":[],"source":["model_name = \"google/flan-t5-small\""]},{"cell_type":"code","execution_count":null,"metadata":{"height":96,"id":"H34zeRMhssMF","outputId":"f28bf62e-037e-4432-cf0a-ca157449708c"},"outputs":[{"name":"stderr","output_type":"stream","text":["You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["import sentencepiece as spm\n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n","\n","tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")"]},{"cell_type":"code","execution_count":null,"metadata":{"height":45,"id":"4_ySnCBessMF"},"outputs":[],"source":["model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")"]},{"cell_type":"code","execution_count":null,"metadata":{"height":30,"id":"axXrbX89ssMG","outputId":"39f3948d-ece0-4544-af2e-ccc31658b5a3"},"outputs":[{"data":{"text/plain":["<bound method Module.get_parameter of T5ForConditionalGeneration(\n","  (shared): Embedding(32128, 512)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 512)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","              (relative_attention_bias): Embedding(32, 6)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-7): 7 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 512)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","              (relative_attention_bias): Embedding(32, 6)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-7): 7 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",")>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["model.get_parameter"]},{"cell_type":"code","execution_count":null,"metadata":{"height":47,"id":"rXI7tOgwssMG","outputId":"620cf175-eda0-4e3e-8b4f-2808c810dfab"},"outputs":[{"name":"stdout","output_type":"stream","text":["T5Stack(\n","  (embed_tokens): Embedding(32128, 512)\n","  (block): ModuleList(\n","    (0): T5Block(\n","      (layer): ModuleList(\n","        (0): T5LayerSelfAttention(\n","          (SelfAttention): T5Attention(\n","            (q): Linear(in_features=512, out_features=384, bias=False)\n","            (k): Linear(in_features=512, out_features=384, bias=False)\n","            (v): Linear(in_features=512, out_features=384, bias=False)\n","            (o): Linear(in_features=384, out_features=512, bias=False)\n","            (relative_attention_bias): Embedding(32, 6)\n","          )\n","          (layer_norm): T5LayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (1): T5LayerCrossAttention(\n","          (EncDecAttention): T5Attention(\n","            (q): Linear(in_features=512, out_features=384, bias=False)\n","            (k): Linear(in_features=512, out_features=384, bias=False)\n","            (v): Linear(in_features=512, out_features=384, bias=False)\n","            (o): Linear(in_features=384, out_features=512, bias=False)\n","          )\n","          (layer_norm): T5LayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (2): T5LayerFF(\n","          (DenseReluDense): T5DenseGatedActDense(\n","            (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","            (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","            (wo): Linear(in_features=1024, out_features=512, bias=False)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (act): NewGELUActivation()\n","          )\n","          (layer_norm): T5LayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (1-7): 7 x T5Block(\n","      (layer): ModuleList(\n","        (0): T5LayerSelfAttention(\n","          (SelfAttention): T5Attention(\n","            (q): Linear(in_features=512, out_features=384, bias=False)\n","            (k): Linear(in_features=512, out_features=384, bias=False)\n","            (v): Linear(in_features=512, out_features=384, bias=False)\n","            (o): Linear(in_features=384, out_features=512, bias=False)\n","          )\n","          (layer_norm): T5LayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (1): T5LayerCrossAttention(\n","          (EncDecAttention): T5Attention(\n","            (q): Linear(in_features=512, out_features=384, bias=False)\n","            (k): Linear(in_features=512, out_features=384, bias=False)\n","            (v): Linear(in_features=512, out_features=384, bias=False)\n","            (o): Linear(in_features=384, out_features=512, bias=False)\n","          )\n","          (layer_norm): T5LayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (2): T5LayerFF(\n","          (DenseReluDense): T5DenseGatedActDense(\n","            (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","            (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","            (wo): Linear(in_features=1024, out_features=512, bias=False)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (act): NewGELUActivation()\n","          )\n","          (layer_norm): T5LayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (final_layer_norm): T5LayerNorm()\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n"]}],"source":["#Decoder\n","print(model.get_decoder())"]},{"cell_type":"code","execution_count":null,"metadata":{"height":47,"id":"-J3QnYUDssMG","outputId":"40d75bd9-a706-4b15-d41e-cfe3f2d056fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Memory footprint: 307844608 bytes\n"]}],"source":["memory_footprint=model.get_memory_footprint()\n","print(f\"Memory footprint: {memory_footprint} bytes\")"]},{"cell_type":"code","execution_count":null,"metadata":{"height":47,"id":"-dV8KvVIssMH","outputId":"e973208a-3d41-4f2a-83ac-f79fa7af53b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["307.0 mb\n"]}],"source":["#memory in mb\n","print(model.get_memory_footprint()//1e+6,\"mb\")"]},{"cell_type":"code","execution_count":null,"metadata":{"height":30,"id":"T9hXBmm8ssMH"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"height":30,"id":"Dq0mesR-ssMH"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"height":98,"id":"rXtqq9hNssMH","outputId":"a348223a-45d8-4c44-f44f-fac764f50215"},"outputs":[{"data":{"text/plain":["tensor([[    0, 10619,     1]])"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["input_text = \"where is Delhi\"\n","inputs= tokenizer(input_text, return_tensors=\"pt\")\n","output = model.generate(**inputs, max_new_tokens=10)\n","output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"height":64,"id":"hUufGlr8ssMH","outputId":"6c35fb61-2ee1-448c-d8c2-3cdcfa72c775"},"outputs":[{"name":"stdout","output_type":"stream","text":["Delhi\n"]}],"source":["# lets decode\n","print(tokenizer.decode(output[0],skip_special_tokens=True))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"height":113,"id":"Z-b7kyJOssMI","outputId":"6704044c-ebd7-4ac9-a3e4-958b126a2dc7"},"outputs":[{"name":"stdout","output_type":"stream","text":["<pad> annie scott</s>\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]}],"source":["input_text = \"Hello, my name is\"\n","input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n","\n","outputs = model.generate(input_ids)\n","print(tokenizer.decode(outputs[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2CwzekTCs2Si"},"outputs":[],"source":["import torch\n","\n","################# monkey patch for quanto\n","def named_module_tensors(module, recurse=False):\n","    for named_parameter in module.named_parameters(recurse=recurse):\n","      name, val = named_parameter\n","      flag = True\n","      if hasattr(val,\"_data\") or hasattr(val,\"_scale\"):\n","        if hasattr(val,\"_data\"):\n","          yield name + \"._data\", val._data\n","        if hasattr(val,\"_scale\"):\n","          yield name + \"._scale\", val._scale\n","      else:\n","        yield named_parameter\n","\n","    for named_buffer in module.named_buffers(recurse=recurse):\n","      yield named_buffer\n","\n","def dtype_byte_size(dtype):\n","    \"\"\"\n","    Returns the size (in bytes) occupied by one parameter of type `dtype`.\n","    \"\"\"\n","    import re\n","    if dtype == torch.bool:\n","        return 1 / 8\n","    bit_search = re.search(r\"[^\\d](\\d+)$\", str(dtype))\n","    if bit_search is None:\n","        raise ValueError(f\"`dtype` is not a valid dtype: {dtype}.\")\n","    bit_size = int(bit_search.groups()[0])\n","    return bit_size // 8\n","\n","def compute_module_sizes(model):\n","    \"\"\"\n","    Compute the size of each submodule of a given model.\n","    \"\"\"\n","    from collections import defaultdict\n","    module_sizes = defaultdict(int)\n","    for name, tensor in named_module_tensors(model, recurse=True):\n","      size = tensor.numel() * dtype_byte_size(tensor.dtype)\n","      name_parts = name.split(\".\")\n","      for idx in range(len(name_parts) + 1):\n","        module_sizes[\".\".join(name_parts[:idx])] += size\n","\n","    return module_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"height":64,"id":"NihWfF3JssMI","outputId":"721507c9-1740-49fe-cf50-fa6f3abe192c"},"outputs":[{"name":"stdout","output_type":"stream","text":["The model size is 0.307844608 GB\n"]}],"source":["module_sizes = compute_module_sizes(model)\n","print(f\"The model size is {module_sizes[''] * 1e-9} GB\")"]},{"cell_type":"code","execution_count":null,"metadata":{"height":53,"id":"dce9Vat-ssMI","outputId":"19a1de05-02bb-433c-b7d7-626cfc5ae0b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["T5ForConditionalGeneration(\n","  (shared): Embedding(32128, 512)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 512)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","              (relative_attention_bias): Embedding(32, 6)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-7): 7 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 512)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","              (relative_attention_bias): Embedding(32, 6)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-7): 7 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",")\n"]}],"source":["#before Qunataization\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"IswZ2-dIssMI"},"source":["## Quantize the model (8-bit precision)"]},{"cell_type":"code","execution_count":null,"metadata":{"height":47,"id":"_QplrnLMssMI"},"outputs":[],"source":["from quanto import quantize, freeze\n","import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"height":30,"id":"wf22AaLbssMI"},"outputs":[],"source":["quantize(model,weights=torch.int8, activations=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"height":30,"id":"S0Qcw3b6ssMJ","outputId":"76a207c6-2300-4123-c536-835559e62fb9"},"outputs":[{"name":"stdout","output_type":"stream","text":["T5ForConditionalGeneration(\n","  (shared): Embedding(32128, 512)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 512)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): QLinear(in_features=512, out_features=384, bias=False)\n","              (k): QLinear(in_features=512, out_features=384, bias=False)\n","              (v): QLinear(in_features=512, out_features=384, bias=False)\n","              (o): QLinear(in_features=384, out_features=512, bias=False)\n","              (relative_attention_bias): Embedding(32, 6)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): QLinear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): QLinear(in_features=512, out_features=1024, bias=False)\n","              (wo): QLinear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-7): 7 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): QLinear(in_features=512, out_features=384, bias=False)\n","              (k): QLinear(in_features=512, out_features=384, bias=False)\n","              (v): QLinear(in_features=512, out_features=384, bias=False)\n","              (o): QLinear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): QLinear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): QLinear(in_features=512, out_features=1024, bias=False)\n","              (wo): QLinear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 512)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): QLinear(in_features=512, out_features=384, bias=False)\n","              (k): QLinear(in_features=512, out_features=384, bias=False)\n","              (v): QLinear(in_features=512, out_features=384, bias=False)\n","              (o): QLinear(in_features=384, out_features=512, bias=False)\n","              (relative_attention_bias): Embedding(32, 6)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): QLinear(in_features=512, out_features=384, bias=False)\n","              (k): QLinear(in_features=512, out_features=384, bias=False)\n","              (v): QLinear(in_features=512, out_features=384, bias=False)\n","              (o): QLinear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): QLinear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): QLinear(in_features=512, out_features=1024, bias=False)\n","              (wo): QLinear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-7): 7 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): QLinear(in_features=512, out_features=384, bias=False)\n","              (k): QLinear(in_features=512, out_features=384, bias=False)\n","              (v): QLinear(in_features=512, out_features=384, bias=False)\n","              (o): QLinear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): QLinear(in_features=512, out_features=384, bias=False)\n","              (k): QLinear(in_features=512, out_features=384, bias=False)\n","              (v): QLinear(in_features=512, out_features=384, bias=False)\n","              (o): QLinear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): QLinear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): QLinear(in_features=512, out_features=1024, bias=False)\n","              (wo): QLinear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): QLinear(in_features=512, out_features=32128, bias=False)\n",")\n"]}],"source":["print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"height":47,"id":"sk2Ie_XBssMJ","outputId":"bcc72d88-4705-4960-c382-5c5d17cc2bb7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Memory footprint: 307845768 bytes\n"]}],"source":["memory_footprint=model.get_memory_footprint()\n","print(f\"Memory footprint: {memory_footprint} bytes\")"]},{"cell_type":"markdown","metadata":{"id":"L2yWSrI3ssMJ"},"source":["### Freeze the model\n","- This will work fine with the smaller T5-Flan model."]},{"cell_type":"code","execution_count":null,"metadata":{"height":30,"id":"zPkOLtmkssMJ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"height":30,"id":"klriV8qJssMJ"},"outputs":[],"source":["freeze(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"height":47,"id":"eKOTwTjSssMJ","outputId":"d70bab6b-db3a-40ca-886c-488bcc1a7816"},"outputs":[{"name":"stdout","output_type":"stream","text":["The model size is 0.12682868 GB\n"]}],"source":["module_sizes = compute_module_sizes(model)\n","print(f\"The model size is {module_sizes[''] * 1e-9} GB\")"]},{"cell_type":"markdown","metadata":{"id":"X4v9WeNCssMJ"},"source":["### Try running inference on the quantized model"]},{"cell_type":"code","execution_count":null,"metadata":{"height":81,"id":"dIgqmd6ossMJ","outputId":"a78c59ce-28a6-416e-cf45-1003b9ca98ab"},"outputs":[{"data":{"text/plain":["tensor([[    0, 10619,     1]])"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["input_text = \"where is Delhi\"\n","inputs= tokenizer(input_text, return_tensors=\"pt\")\n","output = model.generate(**inputs, max_new_tokens=10)\n","output"]},{"cell_type":"code","execution_count":null,"metadata":{"height":30,"id":"FtgT920RssMJ","outputId":"b00597d5-98b1-44f5-e6a3-7799dda4dbd5"},"outputs":[{"name":"stdout","output_type":"stream","text":["<pad> Delhi</s>\n"]}],"source":["print(tokenizer.decode(output[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"height":113,"id":"fzSP4yWhssMK","outputId":"cbfcf35e-19df-49fb-8900-69d5a5a52630"},"outputs":[{"name":"stdout","output_type":"stream","text":["<pad> annie scott</s>\n"]}],"source":["input_text = \"Hello, my name is \"\n","input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n","\n","outputs = model.generate(input_ids)\n","print(tokenizer.decode(outputs[0]))"]},{"cell_type":"markdown","metadata":{"id":"rIkOLWC7ssMK"},"source":["### We see model performs similar even tho the size of the model is reduced."]},{"cell_type":"markdown","metadata":{"id":"XXqjA_9yssMK"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m3YrrdfgssMK"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":0}
